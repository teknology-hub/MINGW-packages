diff --git a/arch/generic/chunkset_c.c b/arch/generic/chunkset_c.c
index 0a585e6..eedaea3 100644
--- a/arch/generic/chunkset_c.c
+++ b/arch/generic/chunkset_c.c
@@ -33,10 +33,10 @@ static inline void storechunk(uint8_t *out, chunk_t *chunk) {
 #define CHUNKCOPY        chunkcopy_c
 #define CHUNKUNROLL      chunkunroll_c
 #define CHUNKMEMSET      chunkmemset_c
-#define CHUNKMEMSET_SAFE chunkmemset_safe_c
+#define CHUNKMEMSET_SAFE PREFIX(chunkmemset_safe_c)
 
 #include "chunkset_tpl.h"
 
-#define INFLATE_FAST     inflate_fast_c
+#define INFLATE_FAST     PREFIX(inflate_fast_c)
 
 #include "inffast_tpl.h"
diff --git a/arch/generic/compare256_c.c b/arch/generic/compare256_c.c
index ad53552..d1a936a 100644
--- a/arch/generic/compare256_c.c
+++ b/arch/generic/compare256_c.c
@@ -17,15 +17,15 @@
 #  define COMPARE256          compare256_16
 #endif
 
-Z_INTERNAL uint32_t compare256_c(const uint8_t *src0, const uint8_t *src1) {
+Z_INTERNAL uint32_t PREFIX(compare256_c)(const uint8_t *src0, const uint8_t *src1) {
     return COMPARE256(src0, src1);
 }
 
 // Generate longest_match_c
-#define LONGEST_MATCH       longest_match_c
+#define LONGEST_MATCH       PREFIX(longest_match_c)
 #include "match_tpl.h"
 
 // Generate longest_match_slow_c
 #define LONGEST_MATCH_SLOW
-#define LONGEST_MATCH       longest_match_slow_c
+#define LONGEST_MATCH       PREFIX(longest_match_slow_c)
 #include "match_tpl.h"
diff --git a/arch/generic/generic_functions.h b/arch/generic/generic_functions.h
index 6e18e34..041a450 100644
--- a/arch/generic/generic_functions.h
+++ b/arch/generic/generic_functions.h
@@ -21,9 +21,9 @@ typedef void     (*slide_hash_func)(deflate_state *s);
 uint32_t adler32_c(uint32_t adler, const uint8_t *buf, size_t len);
 uint32_t adler32_fold_copy_c(uint32_t adler, uint8_t *dst, const uint8_t *src, size_t len);
 
-uint8_t* chunkmemset_safe_c(uint8_t *out, uint8_t *from, unsigned len, unsigned left);
+uint8_t* PREFIX(chunkmemset_safe_c)(uint8_t *out, uint8_t *from, unsigned len, unsigned left);
 
-uint32_t compare256_c(const uint8_t *src0, const uint8_t *src1);
+uint32_t PREFIX(compare256_c)(const uint8_t *src0, const uint8_t *src1);
 
 uint32_t crc32_braid(uint32_t c, const uint8_t *buf, size_t len);
 uint32_t crc32_braid_internal(uint32_t c, const uint8_t *buf, size_t len);
@@ -41,18 +41,18 @@ void     crc32_fold_copy_c(crc32_fold *crc, uint8_t *dst, const uint8_t *src, si
 void     crc32_fold_c(crc32_fold *crc, const uint8_t *src, size_t len, uint32_t init_crc);
 uint32_t crc32_fold_final_c(crc32_fold *crc);
 
-void     inflate_fast_c(PREFIX3(stream) *strm, uint32_t start);
+void     PREFIX(inflate_fast_c)(PREFIX3(stream) *strm, uint32_t start);
 
-uint32_t longest_match_c(deflate_state *const s, Pos cur_match);
-uint32_t longest_match_slow_c(deflate_state *const s, Pos cur_match);
+uint32_t PREFIX(longest_match_c)(deflate_state *const s, Pos cur_match);
+uint32_t PREFIX(longest_match_slow_c)(deflate_state *const s, Pos cur_match);
 
-void     slide_hash_c(deflate_state *s);
+void     PREFIX(slide_hash_c)(deflate_state *s);
 
 #ifdef DISABLE_RUNTIME_CPU_DETECTION
 // Generic code
 #  define native_adler32 adler32_c
 #  define native_adler32_fold_copy adler32_fold_copy_c
-#  define native_chunkmemset_safe chunkmemset_safe_c
+#  define native_chunkmemset_safe PREFIX(chunkmemset_safe_c)
 #ifndef WITHOUT_CHORBA
 #  define native_crc32 crc32_chorba
 #else
@@ -62,11 +62,11 @@ void     slide_hash_c(deflate_state *s);
 #  define native_crc32_fold_copy crc32_fold_copy_c
 #  define native_crc32_fold_final crc32_fold_final_c
 #  define native_crc32_fold_reset crc32_fold_reset_c
-#  define native_inflate_fast inflate_fast_c
-#  define native_slide_hash slide_hash_c
-#  define native_longest_match longest_match_c
-#  define native_longest_match_slow longest_match_slow_c
-#  define native_compare256 compare256_c
+#  define native_inflate_fast PREFIX(inflate_fast_c)
+#  define native_slide_hash PREFIX(slide_hash_c)
+#  define native_longest_match PREFIX(longest_match_c)
+#  define native_longest_match_slow PREFIX(longest_match_slow_c)
+#  define native_compare256 PREFIX(compare256_c)
 #endif
 
 #endif
diff --git a/arch/generic/slide_hash_c.c b/arch/generic/slide_hash_c.c
index 8345b9e..af3101a 100644
--- a/arch/generic/slide_hash_c.c
+++ b/arch/generic/slide_hash_c.c
@@ -44,7 +44,7 @@ static inline void slide_hash_c_chain(Pos *table, uint32_t entries, uint16_t wsi
 #endif /* NOT_TWEAK_COMPILER */
 }
 
-Z_INTERNAL void slide_hash_c(deflate_state *s) {
+Z_INTERNAL void PREFIX(slide_hash_c)(deflate_state *s) {
     uint16_t wsize = (uint16_t)s->w_size;
 
     slide_hash_c_chain(s->head, HASH_SIZE, wsize);
diff --git a/arch/x86/chunkset_avx2.c b/arch/x86/chunkset_avx2.c
index 28deb34..7c5b864 100644
--- a/arch/x86/chunkset_avx2.c
+++ b/arch/x86/chunkset_avx2.c
@@ -119,11 +119,11 @@ static inline halfchunk_t GET_HALFCHUNK_MAG(uint8_t *buf, uint32_t *chunk_rem, u
 #define CHUNKCOPY        chunkcopy_avx2
 #define CHUNKUNROLL      chunkunroll_avx2
 #define CHUNKMEMSET      chunkmemset_avx2
-#define CHUNKMEMSET_SAFE chunkmemset_safe_avx2
+#define CHUNKMEMSET_SAFE PREFIX(chunkmemset_safe_avx2)
 
 #include "chunkset_tpl.h"
 
-#define INFLATE_FAST     inflate_fast_avx2
+#define INFLATE_FAST     PREFIX(inflate_fast_avx2)
 
 #include "inffast_tpl.h"
 
diff --git a/arch/x86/chunkset_avx512.c b/arch/x86/chunkset_avx512.c
index fc27a45..3708ece 100644
--- a/arch/x86/chunkset_avx512.c
+++ b/arch/x86/chunkset_avx512.c
@@ -178,11 +178,11 @@ static inline uint8_t* HALFCHUNKCOPY(uint8_t *out, uint8_t const *from, unsigned
 #define CHUNKSIZE        chunksize_avx512
 #define CHUNKUNROLL      chunkunroll_avx512
 #define CHUNKMEMSET      chunkmemset_avx512
-#define CHUNKMEMSET_SAFE chunkmemset_safe_avx512
+#define CHUNKMEMSET_SAFE PREFIX(chunkmemset_safe_avx512)
 
 #include "chunkset_tpl.h"
 
-#define INFLATE_FAST     inflate_fast_avx512
+#define INFLATE_FAST     PREFIX(inflate_fast_avx512)
 
 #include "inffast_tpl.h"
 
diff --git a/arch/x86/chunkset_sse2.c b/arch/x86/chunkset_sse2.c
index dcfe2c7..686e9b7 100644
--- a/arch/x86/chunkset_sse2.c
+++ b/arch/x86/chunkset_sse2.c
@@ -38,11 +38,11 @@ static inline void storechunk(uint8_t *out, chunk_t *chunk) {
 #define CHUNKCOPY        chunkcopy_sse2
 #define CHUNKUNROLL      chunkunroll_sse2
 #define CHUNKMEMSET      chunkmemset_sse2
-#define CHUNKMEMSET_SAFE chunkmemset_safe_sse2
+#define CHUNKMEMSET_SAFE PREFIX(chunkmemset_safe_sse2)
 
 #include "chunkset_tpl.h"
 
-#define INFLATE_FAST     inflate_fast_sse2
+#define INFLATE_FAST     PREFIX(inflate_fast_sse2)
 
 #include "inffast_tpl.h"
 
diff --git a/arch/x86/chunkset_ssse3.c b/arch/x86/chunkset_ssse3.c
index 7778e52..12d5abc 100644
--- a/arch/x86/chunkset_ssse3.c
+++ b/arch/x86/chunkset_ssse3.c
@@ -57,13 +57,13 @@ static inline chunk_t GET_CHUNK_MAG(uint8_t *buf, uint32_t *chunk_rem, uint32_t
 
 #define CHUNKSIZE        chunksize_ssse3
 #define CHUNKMEMSET      chunkmemset_ssse3
-#define CHUNKMEMSET_SAFE chunkmemset_safe_ssse3
+#define CHUNKMEMSET_SAFE PREFIX(chunkmemset_safe_ssse3)
 #define CHUNKCOPY        chunkcopy_ssse3
 #define CHUNKUNROLL      chunkunroll_ssse3
 
 #include "chunkset_tpl.h"
 
-#define INFLATE_FAST     inflate_fast_ssse3
+#define INFLATE_FAST     PREFIX(inflate_fast_ssse3)
 
 #include "inffast_tpl.h"
 
diff --git a/arch/x86/compare256_avx2.c b/arch/x86/compare256_avx2.c
index 8a0213c..01cb716 100644
--- a/arch/x86/compare256_avx2.c
+++ b/arch/x86/compare256_avx2.c
@@ -46,17 +46,17 @@ static inline uint32_t compare256_avx2_static(const uint8_t *src0, const uint8_t
     return 256;
 }
 
-Z_INTERNAL uint32_t compare256_avx2(const uint8_t *src0, const uint8_t *src1) {
+Z_INTERNAL uint32_t PREFIX(compare256_avx2)(const uint8_t *src0, const uint8_t *src1) {
     return compare256_avx2_static(src0, src1);
 }
 
-#define LONGEST_MATCH       longest_match_avx2
+#define LONGEST_MATCH       PREFIX(longest_match_avx2)
 #define COMPARE256          compare256_avx2_static
 
 #include "match_tpl.h"
 
 #define LONGEST_MATCH_SLOW
-#define LONGEST_MATCH       longest_match_slow_avx2
+#define LONGEST_MATCH       PREFIX(longest_match_slow_avx2)
 #define COMPARE256          compare256_avx2_static
 
 #include "match_tpl.h"
diff --git a/arch/x86/compare256_avx512.c b/arch/x86/compare256_avx512.c
index a1ebe0e..f14e4b5 100644
--- a/arch/x86/compare256_avx512.c
+++ b/arch/x86/compare256_avx512.c
@@ -79,17 +79,17 @@ static inline uint32_t compare256_avx512_static(const uint8_t *src0, const uint8
     return 256;
 }
 
-Z_INTERNAL uint32_t compare256_avx512(const uint8_t *src0, const uint8_t *src1) {
+Z_INTERNAL uint32_t PREFIX(compare256_avx512)(const uint8_t *src0, const uint8_t *src1) {
     return compare256_avx512_static(src0, src1);
 }
 
-#define LONGEST_MATCH       longest_match_avx512
+#define LONGEST_MATCH       PREFIX(longest_match_avx512)
 #define COMPARE256          compare256_avx512_static
 
 #include "match_tpl.h"
 
 #define LONGEST_MATCH_SLOW
-#define LONGEST_MATCH       longest_match_slow_avx512
+#define LONGEST_MATCH       PREFIX(longest_match_slow_avx512)
 #define COMPARE256          compare256_avx512_static
 
 #include "match_tpl.h"
diff --git a/arch/x86/compare256_sse2.c b/arch/x86/compare256_sse2.c
index 25b6531..3f6c7d8 100644
--- a/arch/x86/compare256_sse2.c
+++ b/arch/x86/compare256_sse2.c
@@ -79,17 +79,17 @@ static inline uint32_t compare256_sse2_static(const uint8_t *src0, const uint8_t
     return 256;
 }
 
-Z_INTERNAL uint32_t compare256_sse2(const uint8_t *src0, const uint8_t *src1) {
+Z_INTERNAL uint32_t PREFIX(compare256_sse2)(const uint8_t *src0, const uint8_t *src1) {
     return compare256_sse2_static(src0, src1);
 }
 
-#define LONGEST_MATCH       longest_match_sse2
+#define LONGEST_MATCH       PREFIX(longest_match_sse2)
 #define COMPARE256          compare256_sse2_static
 
 #include "match_tpl.h"
 
 #define LONGEST_MATCH_SLOW
-#define LONGEST_MATCH       longest_match_slow_sse2
+#define LONGEST_MATCH       PREFIX(longest_match_slow_sse2)
 #define COMPARE256          compare256_sse2_static
 
 #include "match_tpl.h"
diff --git a/arch/x86/slide_hash_avx2.c b/arch/x86/slide_hash_avx2.c
index 1000012..a02113b 100644
--- a/arch/x86/slide_hash_avx2.c
+++ b/arch/x86/slide_hash_avx2.c
@@ -30,7 +30,7 @@ static inline void slide_hash_chain(Pos *table, uint32_t entries, const __m256i
     } while (entries > 0);
 }
 
-Z_INTERNAL void slide_hash_avx2(deflate_state *s) {
+Z_INTERNAL void PREFIX(slide_hash_avx2)(deflate_state *s) {
     Assert(s->w_size <= UINT16_MAX, "w_size should fit in uint16_t");
     uint16_t wsize = (uint16_t)s->w_size;
     const __m256i ymm_wsize = _mm256_set1_epi16((short)wsize);
diff --git a/arch/x86/slide_hash_sse2.c b/arch/x86/slide_hash_sse2.c
index 6900a59..769d33e 100644
--- a/arch/x86/slide_hash_sse2.c
+++ b/arch/x86/slide_hash_sse2.c
@@ -51,7 +51,7 @@ next_chain:
     }
 }
 
-Z_INTERNAL void slide_hash_sse2(deflate_state *s) {
+Z_INTERNAL void PREFIX(slide_hash_sse2)(deflate_state *s) {
     Assert(s->w_size <= UINT16_MAX, "w_size should fit in uint16_t");
     uint16_t wsize = (uint16_t)s->w_size;
     const __m128i xmm_wsize = _mm_set1_epi16((short)wsize);
diff --git a/arch/x86/x86_functions.h b/arch/x86/x86_functions.h
index 5d9065e..4cc3e33 100644
--- a/arch/x86/x86_functions.h
+++ b/arch/x86/x86_functions.h
@@ -15,15 +15,15 @@
 #endif
 
 #ifdef X86_SSE2
-uint8_t* chunkmemset_safe_sse2(uint8_t *out, uint8_t *from, unsigned len, unsigned left);
+uint8_t* PREFIX(chunkmemset_safe_sse2)(uint8_t *out, uint8_t *from, unsigned len, unsigned left);
 
 #  ifdef HAVE_BUILTIN_CTZ
-    uint32_t compare256_sse2(const uint8_t *src0, const uint8_t *src1);
-    uint32_t longest_match_sse2(deflate_state *const s, Pos cur_match);
-    uint32_t longest_match_slow_sse2(deflate_state *const s, Pos cur_match);
-    void slide_hash_sse2(deflate_state *s);
+    uint32_t PREFIX(compare256_sse2)(const uint8_t *src0, const uint8_t *src1);
+    uint32_t PREFIX(longest_match_sse2)(deflate_state *const s, Pos cur_match);
+    uint32_t PREFIX(longest_match_slow_sse2)(deflate_state *const s, Pos cur_match);
+    void PREFIX(slide_hash_sse2)(deflate_state *s);
 #  endif
-    void inflate_fast_sse2(PREFIX3(stream)* strm, uint32_t start);
+    void PREFIX(inflate_fast_sse2)(PREFIX3(stream)* strm, uint32_t start);
 #  if !defined(WITHOUT_CHORBA_SSE)
     uint32_t crc32_chorba_sse2(uint32_t crc32, const uint8_t *buf, size_t len);
     uint32_t chorba_small_nondestructive_sse2(uint32_t c, const uint64_t *aligned_buf, size_t aligned_len);
@@ -32,8 +32,8 @@ uint8_t* chunkmemset_safe_sse2(uint8_t *out, uint8_t *from, unsigned len, unsign
 
 #ifdef X86_SSSE3
 uint32_t adler32_ssse3(uint32_t adler, const uint8_t *buf, size_t len);
-uint8_t* chunkmemset_safe_ssse3(uint8_t *out, uint8_t *from, unsigned len, unsigned left);
-void inflate_fast_ssse3(PREFIX3(stream) *strm, uint32_t start);
+uint8_t* PREFIX(chunkmemset_safe_ssse3)(uint8_t *out, uint8_t *from, unsigned len, unsigned left);
+void PREFIX(inflate_fast_ssse3)(PREFIX3(stream) *strm, uint32_t start);
 #endif
 
 #if defined(X86_SSE41) && !defined(WITHOUT_CHORBA_SSE)
@@ -47,25 +47,25 @@ uint32_t adler32_fold_copy_sse42(uint32_t adler, uint8_t *dst, const uint8_t *sr
 #ifdef X86_AVX2
 uint32_t adler32_avx2(uint32_t adler, const uint8_t *buf, size_t len);
 uint32_t adler32_fold_copy_avx2(uint32_t adler, uint8_t *dst, const uint8_t *src, size_t len);
-uint8_t* chunkmemset_safe_avx2(uint8_t *out, uint8_t *from, unsigned len, unsigned left);
+uint8_t* PREFIX(chunkmemset_safe_avx2)(uint8_t *out, uint8_t *from, unsigned len, unsigned left);
 
 #  ifdef HAVE_BUILTIN_CTZ
-    uint32_t compare256_avx2(const uint8_t *src0, const uint8_t *src1);
-    uint32_t longest_match_avx2(deflate_state *const s, Pos cur_match);
-    uint32_t longest_match_slow_avx2(deflate_state *const s, Pos cur_match);
-    void slide_hash_avx2(deflate_state *s);
+    uint32_t PREFIX(compare256_avx2)(const uint8_t *src0, const uint8_t *src1);
+    uint32_t PREFIX(longest_match_avx2)(deflate_state *const s, Pos cur_match);
+    uint32_t PREFIX(longest_match_slow_avx2)(deflate_state *const s, Pos cur_match);
+    void PREFIX(slide_hash_avx2)(deflate_state *s);
 #  endif
-    void inflate_fast_avx2(PREFIX3(stream)* strm, uint32_t start);
+    void PREFIX(inflate_fast_avx2)(PREFIX3(stream)* strm, uint32_t start);
 #endif
 #ifdef X86_AVX512
 uint32_t adler32_avx512(uint32_t adler, const uint8_t *buf, size_t len);
 uint32_t adler32_fold_copy_avx512(uint32_t adler, uint8_t *dst, const uint8_t *src, size_t len);
-uint8_t* chunkmemset_safe_avx512(uint8_t *out, uint8_t *from, unsigned len, unsigned left);
-void inflate_fast_avx512(PREFIX3(stream)* strm, uint32_t start);
+uint8_t* PREFIX(chunkmemset_safe_avx512)(uint8_t *out, uint8_t *from, unsigned len, unsigned left);
+void PREFIX(inflate_fast_avx512)(PREFIX3(stream)* strm, uint32_t start);
 #  ifdef HAVE_BUILTIN_CTZLL
-    uint32_t compare256_avx512(const uint8_t *src0, const uint8_t *src1);
-    uint32_t longest_match_avx512(deflate_state *const s, Pos cur_match);
-    uint32_t longest_match_slow_avx512(deflate_state *const s, Pos cur_match);
+    uint32_t PREFIX(compare256_avx512)(const uint8_t *src0, const uint8_t *src1);
+    uint32_t PREFIX(longest_match_avx512)(deflate_state *const s, Pos cur_match);
+    uint32_t PREFIX(longest_match_slow_avx512)(deflate_state *const s, Pos cur_match);
 #  endif
 #endif
 #ifdef X86_AVX512VNNI
@@ -92,18 +92,18 @@ uint32_t crc32_vpclmulqdq(uint32_t crc32, const uint8_t *buf, size_t len);
 // X86 - SSE2
 #  if (defined(X86_SSE2) && defined(__SSE2__)) || defined(__x86_64__) || defined(_M_X64)
 #    undef native_chunkmemset_safe
-#    define native_chunkmemset_safe chunkmemset_safe_sse2
+#    define native_chunkmemset_safe PREFIX(chunkmemset_safe_sse2)
 #    undef native_inflate_fast
-#    define native_inflate_fast inflate_fast_sse2
+#    define native_inflate_fast PREFIX(inflate_fast_sse2)
 #    undef native_slide_hash
-#    define native_slide_hash slide_hash_sse2
+#    define native_slide_hash PREFIX(slide_hash_sse2)
 #    ifdef HAVE_BUILTIN_CTZ
 #      undef native_compare256
-#      define native_compare256 compare256_sse2
+#      define native_compare256 PREFIX(compare256_sse2)
 #      undef native_longest_match
-#      define native_longest_match longest_match_sse2
+#      define native_longest_match PREFIX(longest_match_sse2)
 #      undef native_longest_match_slow
-#      define native_longest_match_slow longest_match_slow_sse2
+#      define native_longest_match_slow PREFIX(longest_match_slow_sse2)
 #      if !defined(WITHOUT_CHORBA_SSE)
 #        undef native_crc32
 #        define native_crc32 crc32_chorba_sse2
@@ -115,9 +115,9 @@ uint32_t crc32_vpclmulqdq(uint32_t crc32, const uint8_t *buf, size_t len);
 #    undef native_adler32
 #    define native_adler32 adler32_ssse3
 #    undef native_chunkmemset_safe
-#    define native_chunkmemset_safe chunkmemset_safe_ssse3
+#    define native_chunkmemset_safe PREFIX(chunkmemset_safe_ssse3)
 #    undef native_inflate_fast
-#    define native_inflate_fast inflate_fast_ssse3
+#    define native_inflate_fast PREFIX(inflate_fast_ssse3)
 #  endif
 // X86 - SSE4.1
 #  if defined(X86_SSE41) && defined(__SSE4_1__) && !defined(WITHOUT_CHORBA_SSE)
@@ -149,18 +149,18 @@ uint32_t crc32_vpclmulqdq(uint32_t crc32, const uint8_t *buf, size_t len);
 #    undef native_adler32_fold_copy
 #    define native_adler32_fold_copy adler32_fold_copy_avx2
 #    undef native_chunkmemset_safe
-#    define native_chunkmemset_safe chunkmemset_safe_avx2
+#    define native_chunkmemset_safe PREFIX(chunkmemset_safe_avx2)
 #    undef native_inflate_fast
-#    define native_inflate_fast inflate_fast_avx2
+#    define native_inflate_fast PREFIX(inflate_fast_avx2)
 #    undef native_slide_hash
-#    define native_slide_hash slide_hash_avx2
+#    define native_slide_hash PREFIX(slide_hash_avx2)
 #    ifdef HAVE_BUILTIN_CTZ
 #      undef native_compare256
-#      define native_compare256 compare256_avx2
+#      define native_compare256 PREFIX(compare256_avx2)
 #      undef native_longest_match
-#      define native_longest_match longest_match_avx2
+#      define native_longest_match PREFIX(longest_match_avx2)
 #      undef native_longest_match_slow
-#      define native_longest_match_slow longest_match_slow_avx2
+#      define native_longest_match_slow PREFIX(longest_match_slow_avx2)
 #    endif
 #  endif
 // X86 - AVX512 (F,DQ,BW,Vl)
@@ -170,16 +170,16 @@ uint32_t crc32_vpclmulqdq(uint32_t crc32, const uint8_t *buf, size_t len);
 #    undef native_adler32_fold_copy
 #    define native_adler32_fold_copy adler32_fold_copy_avx512
 #    undef native_chunkmemset_safe
-#    define native_chunkmemset_safe chunkmemset_safe_avx512
+#    define native_chunkmemset_safe PREFIX(chunkmemset_safe_avx512)
 #    undef native_inflate_fast
-#    define native_inflate_fast inflate_fast_avx512
+#    define native_inflate_fast PREFIX(inflate_fast_avx512)
 #    ifdef HAVE_BUILTIN_CTZLL
 #      undef native_compare256
-#      define native_compare256 compare256_avx512
+#      define native_compare256 PREFIX(compare256_avx512)
 #      undef native_longest_match
-#      define native_longest_match longest_match_avx512
+#      define native_longest_match PREFIX(longest_match_avx512)
 #      undef native_longest_match_slow
-#      define native_longest_match_slow longest_match_slow_avx512
+#      define native_longest_match_slow PREFIX(longest_match_slow_avx512)
 #    endif
 // X86 - AVX512 (VNNI)
 #    if defined(X86_AVX512VNNI) && defined(__AVX512VNNI__)
diff --git a/deflate.c b/deflate.c
index 9372c21..e2b27d7 100644
--- a/deflate.c
+++ b/deflate.c
@@ -70,15 +70,15 @@ const char PREFIX(deflate_copyright)[] = " deflate 1.3.1 Copyright 1995-2024 Jea
  *  Function prototypes.
  */
 static int deflateStateCheck      (PREFIX3(stream) *strm);
-Z_INTERNAL block_state deflate_stored(deflate_state *s, int flush);
-Z_INTERNAL block_state deflate_fast  (deflate_state *s, int flush);
-Z_INTERNAL block_state deflate_quick (deflate_state *s, int flush);
+Z_INTERNAL block_state PREFIX(deflate_stored)(deflate_state *s, int flush);
+Z_INTERNAL block_state PREFIX(deflate_fast)  (deflate_state *s, int flush);
+Z_INTERNAL block_state PREFIX(deflate_quick) (deflate_state *s, int flush);
 #ifndef NO_MEDIUM_STRATEGY
-Z_INTERNAL block_state deflate_medium(deflate_state *s, int flush);
+Z_INTERNAL block_state PREFIX(deflate_medium)(deflate_state *s, int flush);
 #endif
-Z_INTERNAL block_state deflate_slow  (deflate_state *s, int flush);
-Z_INTERNAL block_state deflate_rle   (deflate_state *s, int flush);
-Z_INTERNAL block_state deflate_huff  (deflate_state *s, int flush);
+Z_INTERNAL block_state PREFIX(deflate_slow)  (deflate_state *s, int flush);
+Z_INTERNAL block_state PREFIX(deflate_rle)   (deflate_state *s, int flush);
+Z_INTERNAL block_state PREFIX(deflate_huff)  (deflate_state *s, int flush);
 static void lm_set_level         (deflate_state *s, int level);
 static void lm_init              (deflate_state *s);
 
@@ -101,31 +101,31 @@ typedef struct config_s {
 
 static const config configuration_table[10] = {
 /*      good lazy nice chain */
-/* 0 */ {0,    0,  0,    0, deflate_stored},  /* store only */
+/* 0 */ {0,    0,  0,    0, PREFIX(deflate_stored)},  /* store only */
 
 #ifdef NO_QUICK_STRATEGY
-/* 1 */ {4,    4,  8,    4, deflate_fast}, /* max speed, no lazy matches */
-/* 2 */ {4,    5, 16,    8, deflate_fast},
+/* 1 */ {4,    4,  8,    4, PREFIX(deflate_fast)}, /* max speed, no lazy matches */
+/* 2 */ {4,    5, 16,    8, PREFIX(deflate_fast)},
 #else
-/* 1 */ {0,    0,  0,    0, deflate_quick},
-/* 2 */ {4,    4,  8,    4, deflate_fast}, /* max speed, no lazy matches */
+/* 1 */ {0,    0,  0,    0, PREFIX(deflate_quick)},
+/* 2 */ {4,    4,  8,    4, PREFIX(deflate_fast)}, /* max speed, no lazy matches */
 #endif
 
 #ifdef NO_MEDIUM_STRATEGY
-/* 3 */ {4,    6, 32,   32, deflate_fast},
-/* 4 */ {4,    4, 16,   16, deflate_slow},  /* lazy matches */
-/* 5 */ {8,   16, 32,   32, deflate_slow},
-/* 6 */ {8,   16, 128, 128, deflate_slow},
+/* 3 */ {4,    6, 32,   32, PREFIX(deflate_fast)},
+/* 4 */ {4,    4, 16,   16, PREFIX(deflate_slow)},  /* lazy matches */
+/* 5 */ {8,   16, 32,   32, PREFIX(deflate_slow)},
+/* 6 */ {8,   16, 128, 128, PREFIX(deflate_slow)},
 #else
-/* 3 */ {4,    6, 16,    6, deflate_medium},
-/* 4 */ {4,   12, 32,   24, deflate_medium},  /* lazy matches */
-/* 5 */ {8,   16, 32,   32, deflate_medium},
-/* 6 */ {8,   16, 128, 128, deflate_medium},
+/* 3 */ {4,    6, 16,    6, PREFIX(deflate_medium)},
+/* 4 */ {4,   12, 32,   24, PREFIX(deflate_medium)},  /* lazy matches */
+/* 5 */ {8,   16, 32,   32, PREFIX(deflate_medium)},
+/* 6 */ {8,   16, 128, 128, PREFIX(deflate_medium)},
 #endif
 
-/* 7 */ {8,   32, 128,  256, deflate_slow},
-/* 8 */ {32, 128, 258, 1024, deflate_slow},
-/* 9 */ {32, 258, 258, 4096, deflate_slow}}; /* max compression */
+/* 7 */ {8,   32, 128,  256, PREFIX(deflate_slow)},
+/* 8 */ {32, 128, 258, 1024, PREFIX(deflate_slow)},
+/* 9 */ {32, 258, 258, 4096, PREFIX(deflate_slow)}}; /* max compression */
 
 /* Note: the deflate() code requires max_lazy >= STD_MIN_MATCH and max_chain >= 4
  * For deflate_fast() (levels <= 3) good is ignored and lazy has a different
@@ -159,7 +159,7 @@ static const config configuration_table[10] = {
  * Allocate a big buffer and divide it up into the various buffers deflate needs.
  * Handles alignment of allocated buffer and alignment of individual buffers.
  */
-Z_INTERNAL deflate_allocs* alloc_deflate(PREFIX3(stream) *strm, int windowBits, int lit_bufsize) {
+static deflate_allocs* alloc_deflate(PREFIX3(stream) *strm, int windowBits, int lit_bufsize) {
     int curr_size = 0;
 
     /* Define sizes */
@@ -522,7 +522,7 @@ int32_t Z_EXPORT PREFIX(deflateResetKeep)(PREFIX3(stream) *strm) {
         strm->adler = ADLER32_INITIAL_VALUE;
     s->last_flush = -2;
 
-    zng_tr_init(s);
+    PREFIX(zng_tr_init)(s);
 
     DEFLATE_RESET_KEEP_HOOK(strm);  /* hook for IBM Z DFLTCC */
 
@@ -585,7 +585,7 @@ int32_t Z_EXPORT PREFIX(deflatePrime)(PREFIX3(stream) *strm, int32_t bits, int32
         else
             s->bi_buf |= (value64 & ((UINT64_C(1) << put) - 1)) << s->bi_valid;
         s->bi_valid += put;
-        zng_tr_flush_bits(s);
+        PREFIX(zng_tr_flush_bits)(s);
         value64 >>= put;
         bits -= put;
     } while (bits);
@@ -978,9 +978,9 @@ int32_t Z_EXPORT PREFIX(deflate)(PREFIX3(stream) *strm, int32_t flush) {
         block_state bstate;
 
         bstate = DEFLATE_HOOK(strm, flush, &bstate) ? bstate :  /* hook for IBM Z DFLTCC */
-                 s->level == 0 ? deflate_stored(s, flush) :
-                 s->strategy == Z_HUFFMAN_ONLY ? deflate_huff(s, flush) :
-                 s->strategy == Z_RLE ? deflate_rle(s, flush) :
+                 s->level == 0 ? PREFIX(deflate_stored)(s, flush) :
+                 s->strategy == Z_HUFFMAN_ONLY ? PREFIX(deflate_huff)(s, flush) :
+                 s->strategy == Z_RLE ? PREFIX(deflate_rle)(s, flush) :
                  (*(configuration_table[s->level].func))(s, flush);
 
         if (bstate == finish_started || bstate == finish_done) {
@@ -1001,9 +1001,9 @@ int32_t Z_EXPORT PREFIX(deflate)(PREFIX3(stream) *strm, int32_t flush) {
         }
         if (bstate == block_done) {
             if (flush == Z_PARTIAL_FLUSH) {
-                zng_tr_align(s);
+                PREFIX(zng_tr_align)(s);
             } else if (flush != Z_BLOCK) { /* FULL_FLUSH or SYNC_FLUSH */
-                zng_tr_stored_block(s, (char*)0, 0L, 0);
+                PREFIX(zng_tr_stored_block)(s, (char*)0, 0L, 0);
                 /* For a full flush, this empty block will be recognized
                  * as a special marker by inflate_sync().
                  */
@@ -1134,13 +1134,13 @@ static void lm_set_level(deflate_state *s, int level) {
      * properly lookup different hash chains to speed up longest_match search. Since hashing
      * method changes depending on the level we cannot put this into functable. */
     if (s->max_chain_length > 1024) {
-        s->update_hash = &update_hash_roll;
-        s->insert_string = &insert_string_roll;
-        s->quick_insert_string = &quick_insert_string_roll;
+        s->update_hash = &PREFIX(update_hash_roll);
+        s->insert_string = &PREFIX(insert_string_roll);
+        s->quick_insert_string = &PREFIX(quick_insert_string_roll);
     } else {
-        s->update_hash = update_hash;
-        s->insert_string = insert_string;
-        s->quick_insert_string = quick_insert_string;
+        s->update_hash = PREFIX(update_hash);
+        s->insert_string = PREFIX(insert_string);
+        s->quick_insert_string = PREFIX(quick_insert_string);
     }
 
     s->level = level;
diff --git a/deflate.h b/deflate.h
index 2655398..8036272 100644
--- a/deflate.h
+++ b/deflate.h
@@ -123,13 +123,13 @@ typedef uint32_t (* update_hash_cb)        (uint32_t h, uint32_t val);
 typedef void     (* insert_string_cb)      (deflate_state *const s, uint32_t str, uint32_t count);
 typedef Pos      (* quick_insert_string_cb)(deflate_state *const s, uint32_t str);
 
-uint32_t update_hash             (uint32_t h, uint32_t val);
-void     insert_string           (deflate_state *const s, uint32_t str, uint32_t count);
-Pos      quick_insert_string     (deflate_state *const s, uint32_t str);
+uint32_t PREFIX(update_hash)             (uint32_t h, uint32_t val);
+void     PREFIX(insert_string)           (deflate_state *const s, uint32_t str, uint32_t count);
+Pos      PREFIX(quick_insert_string)     (deflate_state *const s, uint32_t str);
 
-uint32_t update_hash_roll        (uint32_t h, uint32_t val);
-void     insert_string_roll      (deflate_state *const s, uint32_t str, uint32_t count);
-Pos      quick_insert_string_roll(deflate_state *const s, uint32_t str);
+uint32_t PREFIX(update_hash_roll)        (uint32_t h, uint32_t val);
+void     PREFIX(insert_string_roll)      (deflate_state *const s, uint32_t str, uint32_t count);
+Pos      PREFIX(quick_insert_string_roll)(deflate_state *const s, uint32_t str);
 
 /* Struct for memory allocation handling */
 typedef struct deflate_allocs_s {
@@ -424,14 +424,14 @@ static inline void put_uint64(deflate_state *s, uint64_t lld) {
 
 
 void Z_INTERNAL PREFIX(fill_window)(deflate_state *s);
-void Z_INTERNAL slide_hash_c(deflate_state *s);
+void Z_INTERNAL PREFIX(slide_hash_c)(deflate_state *s);
 
         /* in trees.c */
-void Z_INTERNAL zng_tr_init(deflate_state *s);
-void Z_INTERNAL zng_tr_flush_block(deflate_state *s, char *buf, uint32_t stored_len, int last);
-void Z_INTERNAL zng_tr_flush_bits(deflate_state *s);
-void Z_INTERNAL zng_tr_align(deflate_state *s);
-void Z_INTERNAL zng_tr_stored_block(deflate_state *s, char *buf, uint32_t stored_len, int last);
+void Z_INTERNAL PREFIX(zng_tr_init)(deflate_state *s);
+void Z_INTERNAL PREFIX(zng_tr_flush_block)(deflate_state *s, char *buf, uint32_t stored_len, int last);
+void Z_INTERNAL PREFIX(zng_tr_flush_bits)(deflate_state *s);
+void Z_INTERNAL PREFIX(zng_tr_align)(deflate_state *s);
+void Z_INTERNAL PREFIX(zng_tr_stored_block)(deflate_state *s, char *buf, uint32_t stored_len, int last);
 void Z_INTERNAL PREFIX(flush_pending)(PREFIX3(streamp) strm);
 #define d_code(dist) ((dist) < 256 ? zng_dist_code[dist] : zng_dist_code[256+((dist)>>7)])
 /* Mapping from a distance to a distance code. dist is the distance - 1 and
diff --git a/deflate_fast.c b/deflate_fast.c
index e682697..7ae3ef2 100644
--- a/deflate_fast.c
+++ b/deflate_fast.c
@@ -16,7 +16,7 @@
  * new strings in the dictionary only for unmatched strings or for short
  * matches. It is used only for the fast compression options.
  */
-Z_INTERNAL block_state deflate_fast(deflate_state *s, int flush) {
+Z_INTERNAL block_state PREFIX(deflate_fast)(deflate_state *s, int flush) {
     Pos hash_head;        /* head of the hash chain */
     int bflush = 0;       /* set if current block must be flushed */
     int64_t dist;
@@ -41,7 +41,7 @@ Z_INTERNAL block_state deflate_fast(deflate_state *s, int flush) {
          * dictionary, and set hash_head to the head of the hash chain:
          */
         if (s->lookahead >= WANT_MIN_MATCH) {
-            hash_head = quick_insert_string(s, s->strstart);
+            hash_head = PREFIX(quick_insert_string)(s, s->strstart);
             dist = (int64_t)s->strstart - hash_head;
 
             /* Find the longest match, discarding those <= prev_length.
@@ -73,11 +73,11 @@ Z_INTERNAL block_state deflate_fast(deflate_state *s, int flush) {
                 match_len--; /* string at strstart already in table */
                 s->strstart++;
 
-                insert_string(s, s->strstart, match_len);
+                PREFIX(insert_string)(s, s->strstart, match_len);
                 s->strstart += match_len;
             } else {
                 s->strstart += match_len;
-                quick_insert_string(s, s->strstart + 2 - STD_MIN_MATCH);
+                PREFIX(quick_insert_string)(s, s->strstart + 2 - STD_MIN_MATCH);
 
                 /* If lookahead < STD_MIN_MATCH, ins_h is garbage, but it does not
                  * matter since it will be recomputed at next deflate call.
diff --git a/deflate_huff.c b/deflate_huff.c
index d5a234b..cd64355 100644
--- a/deflate_huff.c
+++ b/deflate_huff.c
@@ -13,7 +13,7 @@
  * For Z_HUFFMAN_ONLY, do not look for matches.  Do not maintain a hash table.
  * (It will be regenerated if this run of deflate switches away from Huffman.)
  */
-Z_INTERNAL block_state deflate_huff(deflate_state *s, int flush) {
+Z_INTERNAL block_state PREFIX(deflate_huff)(deflate_state *s, int flush) {
     int bflush = 0;         /* set if current block must be flushed */
 
     for (;;) {
diff --git a/deflate_medium.c b/deflate_medium.c
index ae7c737..718c8e4 100644
--- a/deflate_medium.c
+++ b/deflate_medium.c
@@ -52,9 +52,9 @@ static void insert_match(deflate_state *s, struct match match) {
         if (UNLIKELY(match.match_length > 0)) {
             if (match.strstart >= match.orgstart) {
                 if (match.strstart + match.match_length - 1 >= match.orgstart) {
-                    insert_string(s, match.strstart, match.match_length);
+                    PREFIX(insert_string)(s, match.strstart, match.match_length);
                 } else {
-                    insert_string(s, match.strstart, match.orgstart - match.strstart + 1);
+                    PREFIX(insert_string)(s, match.strstart, match.orgstart - match.strstart + 1);
                 }
                 match.strstart += match.match_length;
                 match.match_length = 0;
@@ -72,12 +72,12 @@ static void insert_match(deflate_state *s, struct match match) {
 
         if (LIKELY(match.strstart >= match.orgstart)) {
             if (LIKELY(match.strstart + match.match_length - 1 >= match.orgstart)) {
-                insert_string(s, match.strstart, match.match_length);
+                PREFIX(insert_string)(s, match.strstart, match.match_length);
             } else {
-                insert_string(s, match.strstart, match.orgstart - match.strstart + 1);
+                PREFIX(insert_string)(s, match.strstart, match.orgstart - match.strstart + 1);
             }
         } else if (match.orgstart < match.strstart + match.match_length) {
-            insert_string(s, match.orgstart, match.strstart + match.match_length - match.orgstart);
+            PREFIX(insert_string)(s, match.orgstart, match.strstart + match.match_length - match.orgstart);
         }
         match.strstart += match.match_length;
         match.match_length = 0;
@@ -86,7 +86,7 @@ static void insert_match(deflate_state *s, struct match match) {
         match.match_length = 0;
 
         if (match.strstart >= (STD_MIN_MATCH - 2))
-            quick_insert_string(s, match.strstart + 2 - STD_MIN_MATCH);
+            PREFIX(quick_insert_string)(s, match.strstart + 2 - STD_MIN_MATCH);
 
         /* If lookahead < WANT_MIN_MATCH, ins_h is garbage, but it does not
          * matter since it will be recomputed at next deflate call.
@@ -157,7 +157,7 @@ static void fizzle_matches(deflate_state *s, struct match *current, struct match
     }
 }
 
-Z_INTERNAL block_state deflate_medium(deflate_state *s, int flush) {
+Z_INTERNAL block_state PREFIX(deflate_medium)(deflate_state *s, int flush) {
     /* Align the first struct to start on a new cacheline, this allows us to fit both structs in one cacheline */
     ALIGNED_(16) struct match current_match;
                  struct match next_match;
@@ -199,7 +199,7 @@ Z_INTERNAL block_state deflate_medium(deflate_state *s, int flush) {
         } else {
             hash_head = 0;
             if (s->lookahead >= WANT_MIN_MATCH) {
-                hash_head = quick_insert_string(s, s->strstart);
+                hash_head = PREFIX(quick_insert_string)(s, s->strstart);
             }
 
             current_match.strstart = (uint16_t)s->strstart;
@@ -235,7 +235,7 @@ Z_INTERNAL block_state deflate_medium(deflate_state *s, int flush) {
         /* now, look ahead one */
         if (LIKELY(!early_exit && s->lookahead > MIN_LOOKAHEAD && (uint32_t)(current_match.strstart + current_match.match_length) < (s->window_size - MIN_LOOKAHEAD))) {
             s->strstart = current_match.strstart + current_match.match_length;
-            hash_head = quick_insert_string(s, s->strstart);
+            hash_head = PREFIX(quick_insert_string)(s, s->strstart);
 
             next_match.strstart = (uint16_t)s->strstart;
             next_match.orgstart = next_match.strstart;
diff --git a/deflate_p.h b/deflate_p.h
index e803300..f5d5e9a 100644
--- a/deflate_p.h
+++ b/deflate_p.h
@@ -108,7 +108,7 @@ Z_FORCEINLINE static void flush_pending_inline(PREFIX3(stream) *strm) {
     uint32_t len;
     deflate_state *s = strm->state;
 
-    zng_tr_flush_bits(s);
+    PREFIX(zng_tr_flush_bits)(s);
     len = MIN(s->pending, strm->avail_out);
     if (len == 0)
         return;
@@ -172,7 +172,7 @@ Z_FORCEINLINE static unsigned read_buf(PREFIX3(stream) *strm, unsigned char *buf
  * IN assertion: strstart is set to the end of the current match.
  */
 #define FLUSH_BLOCK_ONLY(s, last) { \
-    zng_tr_flush_block(s, (s->block_start >= 0 ? \
+    PREFIX(zng_tr_flush_block)(s, (s->block_start >= 0 ? \
                    (char *)&s->window[(unsigned)s->block_start] : \
                    NULL), \
                    (uint32_t)((int)s->strstart - s->block_start), \
diff --git a/deflate_quick.c b/deflate_quick.c
index d5fd986..2955556 100644
--- a/deflate_quick.c
+++ b/deflate_quick.c
@@ -44,7 +44,7 @@ extern const ct_data static_dtree[D_CODES];
     } \
 }
 
-Z_INTERNAL block_state deflate_quick(deflate_state *s, int flush) {
+Z_INTERNAL block_state PREFIX(deflate_quick)(deflate_state *s, int flush) {
     Pos hash_head;
     int64_t dist;
     unsigned match_len, last;
@@ -86,7 +86,7 @@ Z_INTERNAL block_state deflate_quick(deflate_state *s, int flush) {
         }
 
         if (LIKELY(s->lookahead >= WANT_MIN_MATCH)) {
-            hash_head = quick_insert_string(s, s->strstart);
+            hash_head = PREFIX(quick_insert_string)(s, s->strstart);
             dist = (int64_t)s->strstart - hash_head;
 
             if (dist <= MAX_DIST(s) && dist > 0) {
diff --git a/deflate_rle.c b/deflate_rle.c
index 9e39810..9ced503 100644
--- a/deflate_rle.c
+++ b/deflate_rle.c
@@ -25,7 +25,7 @@
  * one.  Do not maintain a hash table.  (It will be regenerated if this run of
  * deflate switches away from Z_RLE.)
  */
-Z_INTERNAL block_state deflate_rle(deflate_state *s, int flush) {
+Z_INTERNAL block_state PREFIX(deflate_rle)(deflate_state *s, int flush) {
     int bflush = 0;                 /* set if current block must be flushed */
     unsigned char *scan;            /* scan goes up to strend for length of run */
     uint32_t match_len = 0;
diff --git a/deflate_slow.c b/deflate_slow.c
index 4165eea..9fedb4a 100644
--- a/deflate_slow.c
+++ b/deflate_slow.c
@@ -14,7 +14,7 @@
  * evaluation for matches: a match is finally adopted only if there is
  * no better match at the next window position.
  */
-Z_INTERNAL block_state deflate_slow(deflate_state *s, int flush) {
+Z_INTERNAL block_state PREFIX(deflate_slow)(deflate_state *s, int flush) {
     Pos hash_head;           /* head of hash chain */
     int bflush;              /* set if current block must be flushed */
     int64_t dist;
diff --git a/deflate_stored.c b/deflate_stored.c
index eaa16f1..28f3294 100644
--- a/deflate_stored.c
+++ b/deflate_stored.c
@@ -24,7 +24,7 @@
  * copied. It is most efficient with large input and output buffers, which
  * maximizes the opportunities to have a single copy from next_in to next_out.
  */
-Z_INTERNAL block_state deflate_stored(deflate_state *s, int flush) {
+Z_INTERNAL block_state PREFIX(deflate_stored)(deflate_state *s, int flush) {
     /* Smallest worthy block size when not flushing or finishing. By default
      * this is 32K. This can be as small as 507 bytes for memLevel == 1. For
      * large input and output buffers, the stored block size will be larger.
@@ -65,7 +65,7 @@ Z_INTERNAL block_state deflate_stored(deflate_state *s, int flush) {
          * including any pending bits. This also updates the debugging counts.
          */
         last = flush == Z_FINISH && len == left + s->strm->avail_in ? 1 : 0;
-        zng_tr_stored_block(s, (char *)0, 0L, last);
+        PREFIX(zng_tr_stored_block)(s, (char *)0, 0L, last);
 
         /* Replace the lengths in the dummy stored block with len. */
         s->pending -= 4;
@@ -176,7 +176,7 @@ Z_INTERNAL block_state deflate_stored(deflate_state *s, int flush) {
     if (left >= min_block || ((left || flush == Z_FINISH) && flush != Z_NO_FLUSH && s->strm->avail_in == 0 && left <= have)) {
         len = MIN(left, have);
         last = flush == Z_FINISH && s->strm->avail_in == 0 && len == left ? 1 : 0;
-        zng_tr_stored_block(s, (char *)s->window + s->block_start, len, last);
+        PREFIX(zng_tr_stored_block)(s, (char *)s->window + s->block_start, len, last);
         s->block_start += (int)len;
         PREFIX(flush_pending)(s->strm);
     }
diff --git a/functable.c b/functable.c
index 8924f73..717049d 100644
--- a/functable.c
+++ b/functable.c
@@ -17,11 +17,11 @@
 /* Platform has pointer size atomic store */
 #if defined(__GNUC__) || defined(__clang__)
 #  define FUNCTABLE_ASSIGN(VAR, FUNC_NAME) \
-    __atomic_store(&(functable.FUNC_NAME), &(VAR.FUNC_NAME), __ATOMIC_SEQ_CST)
+    __atomic_store(&(PREFIX(functable).FUNC_NAME), &(VAR.FUNC_NAME), __ATOMIC_SEQ_CST)
 #  define FUNCTABLE_BARRIER() __atomic_thread_fence(__ATOMIC_SEQ_CST)
 #elif defined(_MSC_VER)
 #  define FUNCTABLE_ASSIGN(VAR, FUNC_NAME) \
-    _InterlockedExchangePointer((void * volatile *)&(functable.FUNC_NAME), (void *)(VAR.FUNC_NAME))
+    _InterlockedExchangePointer((void * volatile *)&(PREFIX(functable).FUNC_NAME), (void *)(VAR.FUNC_NAME))
 #  if defined(_M_ARM) || defined(_M_ARM64)
 #    define FUNCTABLE_BARRIER() do { \
     _ReadWriteBarrier();  \
@@ -34,7 +34,7 @@
 #else
 #  warning Unable to detect atomic intrinsic support.
 #  define FUNCTABLE_ASSIGN(VAR, FUNC_NAME) \
-    *((void * volatile *)&(functable.FUNC_NAME)) = (void *)(VAR.FUNC_NAME)
+    *((void * volatile *)&(PREFIX(functable).FUNC_NAME)) = (void *)(VAR.FUNC_NAME)
 #  define FUNCTABLE_BARRIER() do { /* Empty */ } while (0)
 #endif
 
@@ -86,25 +86,25 @@ static int init_functable(void) {
     ft.crc32_fold_final = &crc32_fold_final_c;
     ft.crc32_fold_reset = &crc32_fold_reset_c;
 #    ifndef HAVE_BUILTIN_CTZ
-    ft.longest_match = &longest_match_c;
-    ft.longest_match_slow = &longest_match_slow_c;
-    ft.compare256 = &compare256_c;
+    ft.longest_match = &PREFIX(longest_match_c);
+    ft.longest_match_slow = &PREFIX(longest_match_slow_c);
+    ft.compare256 = &PREFIX(compare256_c);
 #    endif
 #  endif
 #else // WITH_ALL_FALLBACKS
     ft.adler32 = &adler32_c;
     ft.adler32_fold_copy = &adler32_fold_copy_c;
-    ft.chunkmemset_safe = &chunkmemset_safe_c;
+    ft.chunkmemset_safe = &PREFIX(chunkmemset_safe_c);
     ft.crc32 = &crc32_braid;
     ft.crc32_fold = &crc32_fold_c;
     ft.crc32_fold_copy = &crc32_fold_copy_c;
     ft.crc32_fold_final = &crc32_fold_final_c;
     ft.crc32_fold_reset = &crc32_fold_reset_c;
-    ft.inflate_fast = &inflate_fast_c;
-    ft.slide_hash = &slide_hash_c;
-    ft.longest_match = &longest_match_c;
-    ft.longest_match_slow = &longest_match_slow_c;
-    ft.compare256 = &compare256_c;
+    ft.inflate_fast = &PREFIX(inflate_fast_c);
+    ft.slide_hash = &PREFIX(slide_hash_c);
+    ft.longest_match = &PREFIX(longest_match_c);
+    ft.longest_match_slow = &PREFIX(longest_match_slow_c);
+    ft.compare256 = &PREFIX(compare256_c);
 #endif
 
     // Select arch-optimized functions
@@ -121,16 +121,16 @@ static int init_functable(void) {
     if (cf.x86.has_sse2)
 #  endif
     {
-        ft.chunkmemset_safe = &chunkmemset_safe_sse2;
+        ft.chunkmemset_safe = &PREFIX(chunkmemset_safe_sse2);
 #  if !defined(WITHOUT_CHORBA_SSE)
         ft.crc32 = &crc32_chorba_sse2;
 #  endif
-        ft.inflate_fast = &inflate_fast_sse2;
-        ft.slide_hash = &slide_hash_sse2;
+        ft.inflate_fast = &PREFIX(inflate_fast_sse2);
+        ft.slide_hash = &PREFIX(slide_hash_sse2);
 #  ifdef HAVE_BUILTIN_CTZ
-        ft.compare256 = &compare256_sse2;
-        ft.longest_match = &longest_match_sse2;
-        ft.longest_match_slow = &longest_match_slow_sse2;
+        ft.compare256 = &PREFIX(compare256_sse2);
+        ft.longest_match = &PREFIX(longest_match_sse2);
+        ft.longest_match_slow = &PREFIX(longest_match_slow_sse2);
 #  endif
     }
 #endif
@@ -138,8 +138,8 @@ static int init_functable(void) {
 #ifdef X86_SSSE3
     if (cf.x86.has_ssse3) {
         ft.adler32 = &adler32_ssse3;
-        ft.chunkmemset_safe = &chunkmemset_safe_ssse3;
-        ft.inflate_fast = &inflate_fast_ssse3;
+        ft.chunkmemset_safe = &PREFIX(chunkmemset_safe_ssse3);
+        ft.inflate_fast = &PREFIX(inflate_fast_ssse3);
     }
 #endif
 
@@ -175,13 +175,13 @@ static int init_functable(void) {
     if (cf.x86.has_avx2 && cf.x86.has_bmi2) {
         ft.adler32 = &adler32_avx2;
         ft.adler32_fold_copy = &adler32_fold_copy_avx2;
-        ft.chunkmemset_safe = &chunkmemset_safe_avx2;
-        ft.inflate_fast = &inflate_fast_avx2;
-        ft.slide_hash = &slide_hash_avx2;
+        ft.chunkmemset_safe = &PREFIX(chunkmemset_safe_avx2);
+        ft.inflate_fast = &PREFIX(inflate_fast_avx2);
+        ft.slide_hash = &PREFIX(slide_hash_avx2);
 #  ifdef HAVE_BUILTIN_CTZ
-        ft.compare256 = &compare256_avx2;
-        ft.longest_match = &longest_match_avx2;
-        ft.longest_match_slow = &longest_match_slow_avx2;
+        ft.compare256 = &PREFIX(compare256_avx2);
+        ft.longest_match = &PREFIX(longest_match_avx2);
+        ft.longest_match_slow = &PREFIX(longest_match_slow_avx2);
 #  endif
     }
 #endif
@@ -190,12 +190,12 @@ static int init_functable(void) {
     if (cf.x86.has_avx512_common) {
         ft.adler32 = &adler32_avx512;
         ft.adler32_fold_copy = &adler32_fold_copy_avx512;
-        ft.chunkmemset_safe = &chunkmemset_safe_avx512;
-        ft.inflate_fast = &inflate_fast_avx512;
+        ft.chunkmemset_safe = &PREFIX(chunkmemset_safe_avx512);
+        ft.inflate_fast = &PREFIX(inflate_fast_avx512);
 #  ifdef HAVE_BUILTIN_CTZLL
-        ft.compare256 = &compare256_avx512;
-        ft.longest_match = &longest_match_avx512;
-        ft.longest_match_slow = &longest_match_slow_avx512;
+        ft.compare256 = &PREFIX(compare256_avx512);
+        ft.longest_match = &PREFIX(longest_match_avx512);
+        ft.longest_match_slow = &PREFIX(longest_match_slow_avx512);
 #  endif
     }
 #endif
@@ -379,71 +379,71 @@ static int force_init_stub(void) {
 
 static uint32_t adler32_stub(uint32_t adler, const uint8_t* buf, size_t len) {
     FUNCTABLE_INIT_ABORT;
-    return functable.adler32(adler, buf, len);
+    return PREFIX(functable).adler32(adler, buf, len);
 }
 
 static uint32_t adler32_fold_copy_stub(uint32_t adler, uint8_t* dst, const uint8_t* src, size_t len) {
     FUNCTABLE_INIT_ABORT;
-    return functable.adler32_fold_copy(adler, dst, src, len);
+    return PREFIX(functable).adler32_fold_copy(adler, dst, src, len);
 }
 
 static uint8_t* chunkmemset_safe_stub(uint8_t* out, uint8_t *from, unsigned len, unsigned left) {
     FUNCTABLE_INIT_ABORT;
-    return functable.chunkmemset_safe(out, from, len, left);
+    return PREFIX(functable).chunkmemset_safe(out, from, len, left);
 }
 
 static uint32_t compare256_stub(const uint8_t* src0, const uint8_t* src1) {
     FUNCTABLE_INIT_ABORT;
-    return functable.compare256(src0, src1);
+    return PREFIX(functable).compare256(src0, src1);
 }
 
 static uint32_t crc32_stub(uint32_t crc, const uint8_t* buf, size_t len) {
     FUNCTABLE_INIT_ABORT;
-    return functable.crc32(crc, buf, len);
+    return PREFIX(functable).crc32(crc, buf, len);
 }
 
 static void crc32_fold_stub(crc32_fold* crc, const uint8_t* src, size_t len, uint32_t init_crc) {
     FUNCTABLE_INIT_ABORT;
-    functable.crc32_fold(crc, src, len, init_crc);
+    PREFIX(functable).crc32_fold(crc, src, len, init_crc);
 }
 
 static void crc32_fold_copy_stub(crc32_fold* crc, uint8_t* dst, const uint8_t* src, size_t len) {
     FUNCTABLE_INIT_ABORT;
-    functable.crc32_fold_copy(crc, dst, src, len);
+    PREFIX(functable).crc32_fold_copy(crc, dst, src, len);
 }
 
 static uint32_t crc32_fold_final_stub(crc32_fold* crc) {
     FUNCTABLE_INIT_ABORT;
-    return functable.crc32_fold_final(crc);
+    return PREFIX(functable).crc32_fold_final(crc);
 }
 
 static uint32_t crc32_fold_reset_stub(crc32_fold* crc) {
     FUNCTABLE_INIT_ABORT;
-    return functable.crc32_fold_reset(crc);
+    return PREFIX(functable).crc32_fold_reset(crc);
 }
 
 static void inflate_fast_stub(PREFIX3(stream) *strm, uint32_t start) {
     FUNCTABLE_INIT_ABORT;
-    functable.inflate_fast(strm, start);
+    PREFIX(functable).inflate_fast(strm, start);
 }
 
 static uint32_t longest_match_stub(deflate_state* const s, Pos cur_match) {
     FUNCTABLE_INIT_ABORT;
-    return functable.longest_match(s, cur_match);
+    return PREFIX(functable).longest_match(s, cur_match);
 }
 
 static uint32_t longest_match_slow_stub(deflate_state* const s, Pos cur_match) {
     FUNCTABLE_INIT_ABORT;
-    return functable.longest_match_slow(s, cur_match);
+    return PREFIX(functable).longest_match_slow(s, cur_match);
 }
 
 static void slide_hash_stub(deflate_state* s) {
     FUNCTABLE_INIT_ABORT;
-    functable.slide_hash(s);
+    PREFIX(functable).slide_hash(s);
 }
 
 /* functable init */
-Z_INTERNAL struct functable_s functable = {
+Z_INTERNAL struct functable_s PREFIX(functable) = {
     force_init_stub,
     adler32_stub,
     adler32_fold_copy_stub,
diff --git a/functable.h b/functable.h
index 91308e5..e1ebbbc 100644
--- a/functable.h
+++ b/functable.h
@@ -40,14 +40,14 @@ struct functable_s {
     void     (* slide_hash)         (deflate_state *s);
 };
 
-Z_INTERNAL extern struct functable_s functable;
+Z_INTERNAL extern struct functable_s PREFIX(functable);
 
 
 /* Explicitly indicate functions are conditionally dispatched.
  */
-#  define FUNCTABLE_INIT if (functable.force_init()) {return Z_VERSION_ERROR;}
-#  define FUNCTABLE_CALL(name) functable.name
-#  define FUNCTABLE_FPTR(name) functable.name
+#  define FUNCTABLE_INIT if (PREFIX(functable).force_init()) {return Z_VERSION_ERROR;}
+#  define FUNCTABLE_CALL(name) PREFIX(functable).name
+#  define FUNCTABLE_FPTR(name) PREFIX(functable).name
 
 #endif
 
diff --git a/gzlib.c b/gzlib.c
index cc9ee78..0855d1f 100644
--- a/gzlib.c
+++ b/gzlib.c
@@ -80,7 +80,7 @@ int Z_INTERNAL gz_buffer_alloc(gz_state *state) {
         out_size = want * 2;  // double output buffer for decompression
     }
 
-    state->buffers = (unsigned char *)zng_alloc_aligned((in_size + out_size), 64);
+    state->buffers = (unsigned char *)PREFIX(zng_alloc_aligned)((in_size + out_size), 64);
     state->in = state->buffers;
     if (out_size) {
         state->out = state->buffers + (in_size); // Outbuffer goes after inbuffer
@@ -98,7 +98,7 @@ int Z_INTERNAL gz_buffer_alloc(gz_state *state) {
 }
 
 void Z_INTERNAL gz_buffer_free(gz_state *state) {
-    zng_free_aligned(state->buffers);
+    PREFIX(zng_free_aligned)(state->buffers);
     state->buffers = NULL;
     state->out = NULL;
     state->in = NULL;
diff --git a/infback.c b/infback.c
index b3494e2..9505bd3 100644
--- a/infback.c
+++ b/infback.c
@@ -44,7 +44,7 @@ int32_t ZNG_CONDEXPORT PREFIX(inflateBackInit)(PREFIX3(stream) *strm, int32_t wi
     if (strm->zfree == NULL)
         strm->zfree = PREFIX(zcfree);
 
-    inflate_allocs *alloc_bufs = alloc_inflate(strm);
+    inflate_allocs *alloc_bufs = PREFIX(alloc_inflate)(strm);
     if (alloc_bufs == NULL)
         return Z_MEM_ERROR;
 
@@ -273,7 +273,7 @@ int32_t Z_EXPORT PREFIX(inflateBack)(PREFIX3(stream) *strm, in_func in, void *in
             state->next = state->codes;
             state->lencode = (const code *)(state->next);
             state->lenbits = 7;
-            ret = zng_inflate_table(CODES, state->lens, 19, &(state->next), &(state->lenbits), state->work);
+            ret = PREFIX(zng_inflate_table)(CODES, state->lens, 19, &(state->next), &(state->lenbits), state->work);
             if (ret) {
                 SET_BAD("invalid code lengths set");
                 break;
@@ -342,14 +342,14 @@ int32_t Z_EXPORT PREFIX(inflateBack)(PREFIX3(stream) *strm, in_func in, void *in
             state->next = state->codes;
             state->lencode = (const code *)(state->next);
             state->lenbits = 10;
-            ret = zng_inflate_table(LENS, state->lens, state->nlen, &(state->next), &(state->lenbits), state->work);
+            ret = PREFIX(zng_inflate_table)(LENS, state->lens, state->nlen, &(state->next), &(state->lenbits), state->work);
             if (ret) {
                 SET_BAD("invalid literal/lengths set");
                 break;
             }
             state->distcode = (const code *)(state->next);
             state->distbits = 9;
-            ret = zng_inflate_table(DISTS, state->lens + state->nlen, state->ndist,
+            ret = PREFIX(zng_inflate_table)(DISTS, state->lens + state->nlen, state->ndist,
                                 &(state->next), &(state->distbits), state->work);
             if (ret) {
                 SET_BAD("invalid distances set");
@@ -515,7 +515,7 @@ int32_t Z_EXPORT PREFIX(inflateBackEnd)(PREFIX3(stream) *strm) {
         return Z_STREAM_ERROR;
 
     /* Free allocated buffers */
-    free_inflate(strm);
+    PREFIX(free_inflate)(strm);
 
     Tracev((stderr, "inflate: end\n"));
     return Z_OK;
diff --git a/inflate.c b/inflate.c
index 8baa725..111e997 100644
--- a/inflate.c
+++ b/inflate.c
@@ -149,7 +149,7 @@ int32_t Z_EXPORT PREFIX(inflateReset2)(PREFIX3(stream) *strm, int32_t windowBits
  * Allocate a big buffer and divide it up into the various buffers inflate needs.
  * Handles alignment of allocated buffer and alignment of individual buffers.
  */
-Z_INTERNAL inflate_allocs* alloc_inflate(PREFIX3(stream) *strm) {
+Z_INTERNAL inflate_allocs* PREFIX(alloc_inflate)(PREFIX3(stream) *strm) {
     int curr_size = 0;
 
     /* Define sizes */
@@ -202,7 +202,7 @@ Z_INTERNAL inflate_allocs* alloc_inflate(PREFIX3(stream) *strm) {
 /* ===========================================================================
  * Free all allocated inflate buffers
  */
-Z_INTERNAL void free_inflate(PREFIX3(stream) *strm) {
+Z_INTERNAL void PREFIX(free_inflate)(PREFIX3(stream) *strm) {
     struct inflate_state *state = (struct inflate_state *)strm->state;
 
     if (state->alloc_bufs != NULL) {
@@ -233,7 +233,7 @@ int32_t ZNG_CONDEXPORT PREFIX(inflateInit2)(PREFIX3(stream) *strm, int32_t windo
     if (strm->zfree == NULL)
         strm->zfree = PREFIX(zcfree);
 
-    inflate_allocs *alloc_bufs = alloc_inflate(strm);
+    inflate_allocs *alloc_bufs = PREFIX(alloc_inflate)(strm);
     if (alloc_bufs == NULL)
         return Z_MEM_ERROR;
 
@@ -248,7 +248,7 @@ int32_t ZNG_CONDEXPORT PREFIX(inflateInit2)(PREFIX3(stream) *strm, int32_t windo
     state->mode = HEAD;     /* to pass state test in inflateReset2() */
     ret = PREFIX(inflateReset2)(strm, windowBits);
     if (ret != Z_OK) {
-        free_inflate(strm);
+        PREFIX(free_inflate)(strm);
     }
     return ret;
 }
@@ -829,7 +829,7 @@ int32_t Z_EXPORT PREFIX(inflate)(PREFIX3(stream) *strm, int32_t flush) {
             state->next = state->codes;
             state->lencode = (const code *)(state->next);
             state->lenbits = 7;
-            ret = zng_inflate_table(CODES, state->lens, 19, &(state->next), &(state->lenbits), state->work);
+            ret = PREFIX(zng_inflate_table)(CODES, state->lens, 19, &(state->next), &(state->lenbits), state->work);
             if (ret) {
                 SET_BAD("invalid code lengths set");
                 break;
@@ -901,14 +901,14 @@ int32_t Z_EXPORT PREFIX(inflate)(PREFIX3(stream) *strm, int32_t flush) {
             state->next = state->codes;
             state->lencode = (const code *)(state->next);
             state->lenbits = 10;
-            ret = zng_inflate_table(LENS, state->lens, state->nlen, &(state->next), &(state->lenbits), state->work);
+            ret = PREFIX(zng_inflate_table)(LENS, state->lens, state->nlen, &(state->next), &(state->lenbits), state->work);
             if (ret) {
                 SET_BAD("invalid literal/lengths set");
                 break;
             }
             state->distcode = (const code *)(state->next);
             state->distbits = 9;
-            ret = zng_inflate_table(DISTS, state->lens + state->nlen, state->ndist,
+            ret = PREFIX(zng_inflate_table)(DISTS, state->lens + state->nlen, state->ndist,
                             &(state->next), &(state->distbits), state->work);
             if (ret) {
                 SET_BAD("invalid distances set");
@@ -1204,7 +1204,7 @@ int32_t Z_EXPORT PREFIX(inflateEnd)(PREFIX3(stream) *strm) {
         return Z_STREAM_ERROR;
 
     /* Free allocated buffers */
-    free_inflate(strm);
+    PREFIX(free_inflate)(strm);
 
     Tracev((stderr, "inflate: end\n"));
     return Z_OK;
@@ -1388,7 +1388,7 @@ int32_t Z_EXPORT PREFIX(inflateCopy)(PREFIX3(stream) *dest, PREFIX3(stream) *sou
     memcpy((void *)dest, (void *)source, sizeof(PREFIX3(stream)));
 
     /* allocate space */
-    inflate_allocs *alloc_bufs = alloc_inflate(dest);
+    inflate_allocs *alloc_bufs = PREFIX(alloc_inflate)(dest);
     if (alloc_bufs == NULL)
         return Z_MEM_ERROR;
     copy = alloc_bufs->state;
diff --git a/inflate.h b/inflate.h
index 1427e90..1ca5a34 100644
--- a/inflate.h
+++ b/inflate.h
@@ -165,7 +165,7 @@ struct ALIGNED_(64) inflate_state {
 };
 
 void Z_INTERNAL PREFIX(fixedtables)(struct inflate_state *state);
-Z_INTERNAL inflate_allocs* alloc_inflate(PREFIX3(stream) *strm);
-Z_INTERNAL void free_inflate(PREFIX3(stream) *strm);
+Z_INTERNAL inflate_allocs* PREFIX(alloc_inflate)(PREFIX3(stream) *strm);
+Z_INTERNAL void PREFIX(free_inflate)(PREFIX3(stream) *strm);
 
 #endif /* INFLATE_H_ */
diff --git a/inftrees.c b/inftrees.c
index 5234fe7..2a218c2 100644
--- a/inftrees.c
+++ b/inftrees.c
@@ -27,7 +27,7 @@ const char PREFIX(inflate_copyright)[] = " inflate 1.3.1 Copyright 1995-2024 Mar
    table index bits.  It will differ if the request is greater than the
    longest code or if it is less than the shortest code.
  */
-int Z_INTERNAL zng_inflate_table(codetype type, uint16_t *lens, unsigned codes,
+int Z_INTERNAL PREFIX(zng_inflate_table)(codetype type, uint16_t *lens, unsigned codes,
                                 code * *table, unsigned *bits, uint16_t *work) {
     unsigned len;               /* a code's length in bits */
     unsigned sym;               /* index of code symbols */
diff --git a/inftrees.h b/inftrees.h
index ad2be15..e62a364 100644
--- a/inftrees.h
+++ b/inftrees.h
@@ -60,7 +60,7 @@ typedef enum {
     DISTS
 } codetype;
 
-int Z_INTERNAL zng_inflate_table (codetype type, uint16_t *lens, unsigned codes,
+int Z_INTERNAL PREFIX(zng_inflate_table) (codetype type, uint16_t *lens, unsigned codes,
                                   code * *table, unsigned *bits, uint16_t *work);
 
 #endif /* INFTREES_H_ */
diff --git a/insert_string.c b/insert_string.c
index 0c72347..22c4737 100644
--- a/insert_string.c
+++ b/insert_string.c
@@ -14,8 +14,8 @@
 #define HASH_CALC_VAR        h
 #define HASH_CALC_VAR_INIT   uint32_t h
 
-#define UPDATE_HASH          update_hash
-#define INSERT_STRING        insert_string
-#define QUICK_INSERT_STRING  quick_insert_string
+#define UPDATE_HASH          PREFIX(update_hash)
+#define INSERT_STRING        PREFIX(insert_string)
+#define QUICK_INSERT_STRING  PREFIX(quick_insert_string)
 
 #include "insert_string_tpl.h"
diff --git a/insert_string_roll.c b/insert_string_roll.c
index 8693f96..31cd252 100644
--- a/insert_string_roll.c
+++ b/insert_string_roll.c
@@ -17,8 +17,8 @@
 #define HASH_CALC_MASK       (32768u - 1u)
 #define HASH_CALC_OFFSET     (STD_MIN_MATCH-1)
 
-#define UPDATE_HASH          update_hash_roll
-#define INSERT_STRING        insert_string_roll
-#define QUICK_INSERT_STRING  quick_insert_string_roll
+#define UPDATE_HASH          PREFIX(update_hash_roll)
+#define INSERT_STRING        PREFIX(insert_string_roll)
+#define QUICK_INSERT_STRING  PREFIX(quick_insert_string_roll)
 
 #include "insert_string_tpl.h"
diff --git a/trees.c b/trees.c
index fd71699..7f80a1a 100644
--- a/trees.c
+++ b/trees.c
@@ -80,7 +80,7 @@ static int  detect_data_type (deflate_state *s);
 /* ===========================================================================
  * Initialize the tree data structures for a new zlib stream.
  */
-void Z_INTERNAL zng_tr_init(deflate_state *s) {
+void Z_INTERNAL PREFIX(zng_tr_init)(deflate_state *s) {
     s->l_desc.dyn_tree = s->dyn_ltree;
     s->l_desc.stat_desc = &static_l_desc;
 
@@ -592,7 +592,7 @@ static void send_all_trees(deflate_state *s, int lcodes, int dcodes, int blcodes
 /* ===========================================================================
  * Send a stored block
  */
-void Z_INTERNAL zng_tr_stored_block(deflate_state *s, char *buf, uint32_t stored_len, int last) {
+void Z_INTERNAL PREFIX(zng_tr_stored_block)(deflate_state *s, char *buf, uint32_t stored_len, int last) {
     /* buf: input block */
     /* stored_len: length of input block */
     /* last: one if this is the last block for a file */
@@ -615,17 +615,17 @@ void Z_INTERNAL zng_tr_stored_block(deflate_state *s, char *buf, uint32_t stored
  * Send one empty static block to give enough lookahead for inflate.
  * This takes 10 bits, of which 7 may remain in the bit buffer.
  */
-void Z_INTERNAL zng_tr_align(deflate_state *s) {
+void Z_INTERNAL PREFIX(zng_tr_align)(deflate_state *s) {
     zng_tr_emit_tree(s, STATIC_TREES, 0);
     zng_tr_emit_end_block(s, static_ltree, 0);
-    zng_tr_flush_bits(s);
+    PREFIX(zng_tr_flush_bits)(s);
 }
 
 /* ===========================================================================
  * Determine the best encoding for the current block: dynamic trees, static
  * trees or store, and write out the encoded block.
  */
-void Z_INTERNAL zng_tr_flush_block(deflate_state *s, char *buf, uint32_t stored_len, int last) {
+void Z_INTERNAL PREFIX(zng_tr_flush_block)(deflate_state *s, char *buf, uint32_t stored_len, int last) {
     /* buf: input block, or NULL if too old */
     /* stored_len: length of input block */
     /* last: one if this is the last block for a file */
@@ -681,7 +681,7 @@ void Z_INTERNAL zng_tr_flush_block(deflate_state *s, char *buf, uint32_t stored_
          * successful. If LIT_BUFSIZE <= WSIZE, it is never too late to
          * transform a block into a stored block.
          */
-        zng_tr_stored_block(s, buf, stored_len, last);
+        PREFIX(zng_tr_stored_block)(s, buf, stored_len, last);
 
     } else if (static_lenb == opt_lenb) {
         zng_tr_emit_tree(s, STATIC_TREES, last);
@@ -794,7 +794,7 @@ static int detect_data_type(deflate_state *s) {
 /* ===========================================================================
  * Flush the bit buffer, keeping at most 7 bits in it.
  */
-void Z_INTERNAL zng_tr_flush_bits(deflate_state *s) {
+void Z_INTERNAL PREFIX(zng_tr_flush_bits)(deflate_state *s) {
     if (s->bi_valid >= 48) {
         put_uint32(s, (uint32_t)s->bi_buf);
         put_short(s, (uint16_t)(s->bi_buf >> 32));
diff --git a/zlib_name_mangling-ng.h.in b/zlib_name_mangling-ng.h.in
index e90904a..261afa8 100644
--- a/zlib_name_mangling-ng.h.in
+++ b/zlib_name_mangling-ng.h.in
@@ -115,8 +115,8 @@
 #define zng_inflateValidate       @ZLIB_SYMBOL_PREFIX@zng_inflateValidate
 #define zng_inflate_copyright     @ZLIB_SYMBOL_PREFIX@zng_inflate_copyright
 #define zng_inflate_ensure_window @ZLIB_SYMBOL_PREFIX@zng_inflate_ensure_window
-#define zng_inflate_fast          @ZLIB_SYMBOL_PREFIX@zng_inflate_fast
-#define zng_inflate_table         @ZLIB_SYMBOL_PREFIX@zng_inflate_table
+#define zng_zng_inflate_fast      @ZLIB_SYMBOL_PREFIX@zng_zng_inflate_fast
+#define zng_zng_inflate_table     @ZLIB_SYMBOL_PREFIX@zng_zng_inflate_table
 #define zng_read_buf              @ZLIB_SYMBOL_PREFIX@zng_read_buf
 #define zng_uncompress            @ZLIB_SYMBOL_PREFIX@zng_uncompress
 #define zng_uncompress2           @ZLIB_SYMBOL_PREFIX@zng_uncompress2
@@ -162,8 +162,8 @@
 #define zng_vstring            @ZLIB_SYMBOL_PREFIX@zng_vstring
 #define zng_zError             @ZLIB_SYMBOL_PREFIX@zng_zError
 
-#define zng_alloc_aligned      @ZLIB_SYMBOL_PREFIX@zng_alloc_aligned
-#define zng_free_aligned       @ZLIB_SYMBOL_PREFIX@zng_free_aligned
+#define zng_zng_alloc_aligned  @ZLIB_SYMBOL_PREFIX@zng_zng_alloc_aligned
+#define zng_zng_free_aligned   @ZLIB_SYMBOL_PREFIX@zng_zng_free_aligned
 #define zng_get_crc_table      @ZLIB_SYMBOL_PREFIX@zng_get_crc_table
 #define zng_inflateSyncPoint   @ZLIB_SYMBOL_PREFIX@zng_inflateSyncPoint
 #define zng_inflateUndermine   @ZLIB_SYMBOL_PREFIX@zng_inflateUndermine
diff --git a/zutil.c b/zutil.c
index 89bd1f4..2fa5d78 100644
--- a/zutil.c
+++ b/zutil.c
@@ -113,7 +113,7 @@ void Z_INTERNAL PREFIX(zcfree)(void *opaque, void *ptr) {
 }
 
 /* Provide aligned allocations, only used by gz* code */
-void Z_INTERNAL *zng_alloc_aligned(unsigned size, unsigned align) {
+void Z_INTERNAL *PREFIX(zng_alloc_aligned)(unsigned size, unsigned align) {
     uintptr_t return_ptr, original_ptr;
     uint32_t alloc_size, align_diff;
     void *ptr;
@@ -138,7 +138,7 @@ void Z_INTERNAL *zng_alloc_aligned(unsigned size, unsigned align) {
     return (void *)return_ptr;
 }
 
-void Z_INTERNAL zng_free_aligned(void *ptr) {
+void Z_INTERNAL PREFIX(zng_free_aligned)(void *ptr) {
     if (!ptr)
         return;
 
diff --git a/zutil.h b/zutil.h
index 9017c68..f698819 100644
--- a/zutil.h
+++ b/zutil.h
@@ -133,8 +133,8 @@ extern z_const char * const PREFIX(z_errmsg)[10]; /* indexed by 2-zlib_error */
 
 void Z_INTERNAL *PREFIX(zcalloc)(void *opaque, unsigned items, unsigned size);
 void Z_INTERNAL  PREFIX(zcfree)(void *opaque, void *ptr);
-void Z_INTERNAL *zng_alloc_aligned(unsigned size, unsigned align);
-void Z_INTERNAL zng_free_aligned(void *ptr);
+void Z_INTERNAL *PREFIX(zng_alloc_aligned)(unsigned size, unsigned align);
+void Z_INTERNAL PREFIX(zng_free_aligned)(void *ptr);
 
 typedef void *zng_calloc_func(void *opaque, unsigned items, unsigned size);
 typedef void  zng_cfree_func(void *opaque, void *ptr);
